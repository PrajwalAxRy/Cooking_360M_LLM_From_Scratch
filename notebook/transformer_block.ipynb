{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3abbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d43f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from llm_foundry.model.layers import RMSNorm, FeedForward\n",
    "from llm_foundry.model.attention import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b59519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOverview of the LLM architecture being built here.\\n\\nInput -> Tokenization -> Embedding -> Stack of Transformer Blocks -> Output Layer\\n\\nTransformer Block:\\n    - Input\\n    - Residual Connection + Layer Norm -> Multi-Head Attention (with RoPE for positional encoding) -> Dropout -> Add Input (shortcut connection)\\n    - Residual Connection + Layer Norm -> Feed-Forward Network -> Dropout -> Add Previous (shortcut connection)\\n    - Output\\n\\nThe key improvements:\\n1. Shortcut connections (residual connections) around both attention and FFN components\\n2. Multiple stacked transformer blocks for deep representation\\n3. RoPE (Rotary Position Embedding) integrated directly into attention mechanism \\n   instead of separate positional encoding\\n4. Layer normalization before attention and FFN (Pre-LN architecture)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Laying down the overall architecture of the LLM.\n",
    "\"\"\"\n",
    "Overview of the LLM architecture being built here.\n",
    "\n",
    "Input -> Tokenization -> Embedding -> Stack of Transformer Blocks -> Output Layer\n",
    "\n",
    "Transformer Block:\n",
    "    - Input\n",
    "    - Residual Connection + Layer Norm -> Multi-Head Attention (with RoPE for positional encoding) -> Dropout -> Add Input (shortcut connection)\n",
    "    - Residual Connection + Layer Norm -> Feed-Forward Network -> Dropout -> Add Previous (shortcut connection)\n",
    "    - Output\n",
    "\n",
    "The key improvements:\n",
    "1. Shortcut connections (residual connections) around both attention and FFN components\n",
    "2. Multiple stacked transformer blocks for deep representation\n",
    "3. RoPE (Rotary Position Embedding) integrated directly into attention mechanism \n",
    "   instead of separate positional encoding\n",
    "4. Layer normalization before attention and FFN (Pre-LN architecture)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da47adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_foundry.utils.config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93ca9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'model', 'training'])\n",
      "dict_keys(['vocab_size', 'context_length', 'emb_dim', 'n_heads', 'n_layers', 'hidden_dim', 'head_dim', 'qk_norm', 'n_kv_groups', 'rope_local_base', 'rope_base', 'layer_types', 'dtype', 'query_pre_attn_scalar'])\n"
     ]
    }
   ],
   "source": [
    "config = load_config(path=\"../configs/llm_270m.yaml\")\n",
    "\n",
    "print(config.keys())\n",
    "print(config[\"model\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69a6be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 50257\n",
      "context_length: 1024\n",
      "emb_dim: 1024\n",
      "n_heads: 8\n",
      "n_layers: 12\n",
      "hidden_dim: 2048\n",
      "head_dim: 256\n",
      "qk_norm: True\n",
      "n_kv_groups: 8\n",
      "rope_local_base: 10000.0\n",
      "rope_base: 1000000.0\n",
      "layer_types: ['full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention']\n",
      "dtype: bfloat16\n",
      "query_pre_attn_scalar: 256\n"
     ]
    }
   ],
   "source": [
    "model_cfg = config[\"model\"]\n",
    "\n",
    "for key, value in model_cfg.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef612744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.randint(0, model_cfg[\"vocab_size\"], (2, 8))  # (batch_size, seq_length)\n",
    "\n",
    "print(\"Input IDs shape:\", input_ids.shape)  # Should be (2, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb9828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings shape: torch.Size([2, 8, 1024])\n",
      "Scaled Embeddings shape: torch.Size([2, 8, 1024])\n"
     ]
    }
   ],
   "source": [
    "tkn_emb = nn.Embedding(model_cfg[\"vocab_size\"], model_cfg[\"emb_dim\"])\n",
    "\n",
    "x = tkn_emb(input_ids)  # (batch_size, seq_length, emb_dim)\n",
    "print(\"Token Embeddings shape:\", x.shape)  # Should be (2, 8, emb_dim)\n",
    "\n",
    "## Scale the input independent of the embedding dimension\n",
    "x = x * (model_cfg[\"emb_dim\"] ** 0.5)\n",
    "print(\"Scaled Embeddings shape:\", x.shape)  # Should be (2, 8, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "390cbaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention\n",
    "\n",
    "att = MultiHeadAttention(\n",
    "    d_in=model_cfg[\"emb_dim\"],\n",
    "    num_heads=model_cfg[\"n_heads\"],\n",
    "    head_dim=model_cfg[\"head_dim\"],\n",
    "    qk_norm=model_cfg[\"qk_norm\"],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "## FF Layer\n",
    "ff = FeedForward(model_cfg)\n",
    "\n",
    "## Norm Layer\n",
    "input_layer_RMSNorm = RMSNorm(model_cfg[\"emb_dim\"], eps=1e-6)\n",
    "post_att_layer_RMSNorm = RMSNorm(model_cfg[\"emb_dim\"], eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff48b45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Attention shape: torch.Size([2, 8, 1024])\n"
     ]
    }
   ],
   "source": [
    "## Running Attention\n",
    "shortcut = x\n",
    "x_norm = input_layer_RMSNorm(x)\n",
    "x_att = att(x_norm, mask=True)  # (batch_size, seq_length, emb_dim)\n",
    "\n",
    "x = shortcut + x_att  # Residual connection\n",
    "print(\"Post-Attention shape:\", x.shape)  # Should be (2, 8, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081d3825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-FF shape: torch.Size([2, 8, 1024])\n"
     ]
    }
   ],
   "source": [
    "## Running FF\n",
    "shortcut = x\n",
    "x_norm = post_att_layer_RMSNorm(x).to(torch.bfloat16)\n",
    "x_ff = ff(x_norm)  # (batch_size, seq_length, emb_dim)\n",
    "x = shortcut + x_ff  # Residual connection\n",
    "\n",
    "print(\"Post-FF shape:\", x.shape)  # Should be (2, 8, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f561e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stacking multiple transformer blocks\n",
    "block = [\n",
    "    {\n",
    "        \"att\": MultiHeadAttention(model_cfg[\"emb_dim\"], model_cfg[\"n_heads\"], model_cfg[\"head_dim\"], qk_norm=model_cfg[\"qk_norm\"]),\n",
    "        \"ff\": FeedForward(model_cfg),\n",
    "        \"ln1\": RMSNorm(model_cfg[\"emb_dim\"], eps=1e-6),\n",
    "        \"ln2\": RMSNorm(model_cfg[\"emb_dim\"], eps=1e-6)\n",
    "    }\n",
    "    for number_of_layers in range(model_cfg[\"n_layers\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde4f4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'att': MultiHeadAttention(\n",
       "   (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "   (W_key): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "   (W_value): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "   (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "   (q_norm): RMSNorm()\n",
       "   (k_norm): RMSNorm()\n",
       " ),\n",
       " 'ff': FeedForward(\n",
       "   (fc1): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "   (fc2): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "   (fc3): Linear(in_features=2048, out_features=1024, bias=False)\n",
       " ),\n",
       " 'ln1': RMSNorm(),\n",
       " 'ln2': RMSNorm()}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb1ef2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Block 1, shape: torch.Size([2, 8, 1024])\n",
      "After Block 2, shape: torch.Size([2, 8, 1024])\n",
      "After Block 3, shape: torch.Size([2, 8, 1024])\n",
      "After Block 4, shape: torch.Size([2, 8, 1024])\n",
      "After Block 5, shape: torch.Size([2, 8, 1024])\n",
      "After Block 6, shape: torch.Size([2, 8, 1024])\n",
      "After Block 7, shape: torch.Size([2, 8, 1024])\n",
      "After Block 8, shape: torch.Size([2, 8, 1024])\n",
      "After Block 9, shape: torch.Size([2, 8, 1024])\n",
      "After Block 10, shape: torch.Size([2, 8, 1024])\n",
      "After Block 11, shape: torch.Size([2, 8, 1024])\n",
      "After Block 12, shape: torch.Size([2, 8, 1024])\n"
     ]
    }
   ],
   "source": [
    "x = tkn_emb(input_ids)* (model_cfg[\"emb_dim\"] ** 0.5)\n",
    "\n",
    "for i, block in enumerate(block):\n",
    "    # Attention\n",
    "    shortcut = x\n",
    "    x_norm = block[\"ln1\"](x)\n",
    "    x_att = block[\"att\"](x_norm, mask=True)\n",
    "    x = shortcut + x_att  # Residual connection\n",
    "\n",
    "    # Feed Forward\n",
    "    shortcut = x\n",
    "    x_norm = block[\"ln2\"](x).to(torch.bfloat16)\n",
    "    x_ff = block[\"ff\"](x_norm)\n",
    "    x = shortcut + x_ff  # Residual connection\n",
    "\n",
    "    print(f\"After Block {i+1}, shape: {x.shape}\")  # Should be (2, 8, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd37caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b18ef780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 8, 50257])\n"
     ]
    }
   ],
   "source": [
    "## Final Layer Norm\n",
    "final_norm = RMSNorm(model_cfg[\"emb_dim\"], eps=1e-6)\n",
    "out_head = nn.Linear(model_cfg[\"emb_dim\"], model_cfg[\"vocab_size\"])\n",
    "\n",
    "x = final_norm(x)\n",
    "logits = out_head(x)  # (batch_size, seq_length, vocab_size)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)  # Should be (2, 8, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd041064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.239240646362305\n"
     ]
    }
   ],
   "source": [
    "### Computing Loss\n",
    "\n",
    "targets = torch.randint(0, model_cfg[\"vocab_size\"], (2, 8))     # (batch_size, seq_length)\n",
    "\n",
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8e613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3888f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [\n",
    "    {\n",
    "        \"att\": MultiHeadAttention(model_cfg[\"emb_dim\"], model_cfg[\"n_heads\"], model_cfg[\"head_dim\"], qk_norm=model_cfg[\"qk_norm\"]),\n",
    "        \"ff\": FeedForward(model_cfg),\n",
    "        \"ln1\": RMSNorm(model_cfg[\"emb_dim\"], eps=1e-6),\n",
    "        \"ln2\": RMSNorm(model_cfg[\"emb_dim\"], eps=1e-6)\n",
    "    }\n",
    "    for _ in range(model_cfg[\"n_layers\"])  # Fixed: use '_' instead of 'number_of_layers'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ce30c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Loop\n",
    "def generate(input_ids, max_new_tokens, blocks, temperature=1.0, top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = tkn_emb(input_ids) * (model_cfg[\"emb_dim\"] ** 0.5)\n",
    "        for block in blocks:\n",
    "            sc = x\n",
    "            x_norm = block[\"ln1\"](x)\n",
    "            x_att = block[\"att\"](x_norm, mask=None)\n",
    "            x = sc + x_att\n",
    "\n",
    "            sc = x\n",
    "            x_norm = block[\"ln2\"](x).to(torch.bfloat16)\n",
    "            x_ff = block[\"ff\"](x_norm)\n",
    "            x = sc + x_ff\n",
    "\n",
    "        x = final_norm(x)\n",
    "        logits = out_head(x[:, -1, :]) / temperature  # Focus on the last token only\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, index = torch.topk(logits, top_k)\n",
    "            # Smallest values for each.\n",
    "            smallest = v[:, -1].unsqueeze(1)\n",
    "            logits[logits < smallest] = -float('Inf')\n",
    "\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat((input_ids, next_token), dim=1)\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1c53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17ac6b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: tensor([[24160, 14289, 15789, 45289,  8577, 10633, 46144, 39431, 26047, 25751]])\n"
     ]
    }
   ],
   "source": [
    "start_ids = torch.randint(0, model_cfg[\"vocab_size\"], (1, 5))\n",
    "print(\"Generated:\", generate(start_ids, max_new_tokens=5, blocks=blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d0912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
